{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:24:12.786273Z",
     "start_time": "2021-06-05T04:24:12.743476Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the code\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "sys.path.append('../code')\n",
    "\n",
    "# import package functions\n",
    "from script_utils_CNV import get_CNVconfig, show_output\n",
    "from plot import plot_cov\n",
    "from script_utils_CNV import show_output\n",
    "from rollingCov import rolling_coverage\n",
    "from combineCNV import filter_cov\n",
    "\n",
    "\n",
    "# HOME\n",
    "home = '/Users/mahtin'\n",
    "home = '/Users/martinscience'\n",
    "\n",
    "# standard paths\n",
    "static = os.path.join(home, \"Dropbox/Icke/Work/static\")\n",
    "tooldata = os.path.join(home, \"Dropbox/Icke/Work/somVar/tooldata\")\n",
    "testdata = os.path.join(home,\"Dropbox/Icke/Work/somVar/testdata\")\n",
    "PON_path = os.path.join(static, \"PON/HAEv7_hg38_NovaSeq\")\n",
    " \n",
    "cnvdata = os.path.join(tooldata, \"myCNVdata\")\n",
    "output_path = os.path.join(cnvdata, \"output\")\n",
    "plot_path = os.path.join(cnvdata, \"plot\")\n",
    "fig_path = os.path.join(cnvdata, \"figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the config\n",
    "+ use the get_CNVconfig util function to update the general configs with the appropriate paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:24:13.974555Z",
     "start_time": "2021-06-05T04:24:13.967653Z"
    }
   },
   "outputs": [],
   "source": [
    "path_config = dict(\n",
    "        mawk_path=\"../shell\",\n",
    "        cov_path=os.path.join(output_path, \"pile2CNV\"),   # path containing rawcov.gz files for this sample\n",
    "        snp_path=os.path.join(output_path, \"pile2CNV\"),   # path containing snp files for this sample\n",
    "        bed_file=os.path.join(static, \"bed_files/SureSelect/hg38/SS_HAEv7_hg38_Padded.bed\"),\n",
    "        genome_split_path=os.path.join(static, \"genome/gatk/hg38/split\"),\n",
    "        gc_split_path=os.path.join(static, \"genome/gatk/hg38/split\"),\n",
    "        genmap_split_path=os.path.join(static, \"annotation/genmap/hg38/split\"),\n",
    "        PON_path = PON_path,\n",
    "    )\n",
    "CNVconfig = get_CNVconfig(\n",
    "    \"../config/config_CNV.yaml\", \n",
    "    local_config=path_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coverage data and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:37:39.704196Z",
     "start_time": "2021-06-05T04:37:16.320320Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"03_A-B\"\n",
    "cov_rawdf = pd.read_csv(os.path.join(output_path, f\"cov/{sample}.cov.gz\"), sep=\"\\t\", compression=\"gzip\")\n",
    "cov_rawdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:40:56.310616Z",
     "start_time": "2021-06-05T04:40:44.522279Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_params = dict(\n",
    "    figsize=(42,6),\n",
    "    colormap='coolwarm_r',\n",
    "    color_chroms=True,\n",
    "    ylim=(-2,3)\n",
    ")\n",
    "log2 = dict(\n",
    "        title='log2ratio',\n",
    "        plot_type='scatter',   # ['line', 'scatter']\n",
    "        data='log2ratio2',\n",
    "        plot_args=dict(\n",
    "            linewidth=0.3,\n",
    "            color='black',\n",
    "            s=.5,\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "\n",
    "fig, ax, _, chrom_df = plot_cov(cov_rawdf, plots=[log2], chroms='all', region='', **fig_params)\n",
    "# fig.savefig(os.path.join(cnvdata, \"figures/03_GC_cov.jpeg\"))\n",
    "# cov_noGC_df = pd.read_csv(os.path.join(output_path, f\"cov/{sample}.noGC.cov.gz\"), sep=\"\\t\", compression=\"gzip\")\n",
    "# fig.savefig(os.path.join(cnvdata, \"figures/03_GC_cov.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter coverage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:42:26.011299Z",
     "start_time": "2021-06-05T04:42:17.304082Z"
    }
   },
   "outputs": [],
   "source": [
    "CNVconfig = get_CNVconfig(\n",
    "    \"../config/config_CNV.yaml\", \n",
    "    local_config=path_config)\n",
    "\n",
    "cov_df = filter_cov(cov_rawdf, config=CNVconfig)\n",
    "\n",
    "fig, ax, _, chrom_df = plot_cov(cov_df, plots=[log2], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:43:55.875640Z",
     "start_time": "2021-06-05T04:42:47.418459Z"
    }
   },
   "outputs": [],
   "source": [
    "from rollingCov import rolling_coverage\n",
    "roll_df = rolling_coverage(cov_df, config=CNVconfig)\n",
    "\n",
    "log2mean = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'ascatter']\n",
    "        data='log2ratio2_mean',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llh = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covLLH2_sum',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llhdiff = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covLLH2_sumDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='green',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "log2L = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratio2_meanL',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='white',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2R = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratio2_meanR',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='black',\n",
    "            alpha=.7\n",
    "        ))\n",
    "r1 = 'chr20:47Mb-57Mb'\n",
    "r2 = 'chr4:110Mb-140Mb'\n",
    "fig, _, _, _ = plot_cov(roll_df, plots=[log2,log2mean, llh, llhdiff], chroms='all', region=r1, **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:51:30.159528Z",
     "start_time": "2021-06-05T04:43:57.400880Z"
    }
   },
   "outputs": [],
   "source": [
    " roll_df.to_csv(os.path.join(output_path, f\"cov/{sample}.roll.cov.gz\"), sep=\"\\t\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step\n",
    "+ make a copy for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:27:15.408821Z",
     "start_time": "2021-06-05T04:27:15.129537Z"
    }
   },
   "outputs": [],
   "source": [
    "df = cov_df.loc[:,['Chr', 'Pos', 'ExonPos', 'FullExonPos', 'Cov1', 'log2ratio1', 'Cov2', 'log2ratio2']].copy()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assumption: \n",
    "+ log2ratio is normal-distributed around mean value:\n",
    "+ log-likelihood should be far below average at CNV areas\n",
    "### compute the llh for center mass for entire sample\n",
    "+ get the global mean\n",
    "+ compute loglikelihood\n",
    "+ rolling sum for chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:27:24.452286Z",
     "start_time": "2021-06-05T04:27:22.972599Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "col = \"log2ratio1\"\n",
    "sigma = df.query(f'-0.5 < {col} < .5')[col].std() * .9\n",
    "# get the mean of the center band\n",
    "mean = df.query(f'-0.5 < {col} < .5')[col].mean()\n",
    "print(\"sigma:\", sigma)\n",
    "print(\"mean:\", mean)\n",
    "def llh(data, mean, sigma):\n",
    "    s = np.sqrt(2 * np.pi) * sigma\n",
    "    return np.exp((data - mean)**2 / (-2*(sigma**2))) / s\n",
    "\n",
    "r = np.linspace(-2,2,10000)\n",
    "fig1, ax = plt.subplots(figsize=(10,4))\n",
    "_ = ax.scatter(r, llh(r, mean, sigma), s=.5, alpha=0.5)\n",
    "_ = ax.scatter(df[col], rnd.random(len(df.index))*4, s=.01, alpha=.1)\n",
    "_ = ax.set_xlim(-1.5,2)\n",
    "fig1.savefig(os.path.join(plot_path, f\"{sample}.log2gauss.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:27:25.471677Z",
     "start_time": "2021-06-05T04:27:25.465092Z"
    }
   },
   "outputs": [],
   "source": [
    "def llh(data, mean=0.5, sigma=2):\n",
    "    '''\n",
    "    compute llh for one col\n",
    "    '''\n",
    "    s = np.sqrt(2 * np.pi) * sigma\n",
    "    return np.exp((data - mean)**2 / (-2*(sigma**2))) / s\n",
    "\n",
    "\n",
    "def compute_covLLH_col(df, log_col, params={}):\n",
    "    \"\"\"\n",
    "    computes the local log-likelihood of belonging to the center gaussian for specified log_column\n",
    "    \"\"\"\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    min_log2ratio, max_log2ratio = params[\"center_range\"]\n",
    "    # get the sigma and mean of the center band log2ratio\n",
    "    center_logs = df.query(f\"@min_log2ratio < {log_col} < @max_log2ratio\")[log_col]\n",
    "    sigma = center_logs.std() * params[\"sigma_factor\"]\n",
    "    mean = center_logs.mean()\n",
    "    show_output(\n",
    "        f\"Computing log-likelihood of {log_col} belonging to center gaussian [mean:{round(mean, 3)}, sigma:{round(sigma,3)}]\"\n",
    "    )\n",
    "    llh_col = log_col.replace(\"log2ratio\", \"covLLH\")\n",
    "    df.loc[:, llh_col] = llh(df[log_col], mean=mean, sigma=sigma)\n",
    "    # get col index of log_col for adjacent inserting of llh_col\n",
    "\n",
    "    insert_index = df.columns.get_loc(log_col) + 1\n",
    "    df = df.loc[:, cols[:insert_index] + [llh_col] + cols[insert_index:]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_covLLH(df, config={}):\n",
    "    '''\n",
    "    computes the local log-likelihood of belonging to the center gaussian\n",
    "    '''\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"log2ratio\"):\n",
    "            df = compute_covLLH_col(df, col, params=config['rolling']['cov']['LLH'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:50:09.946853Z",
     "start_time": "2021-06-01T11:50:08.269025Z"
    }
   },
   "outputs": [],
   "source": [
    "llh_df = compute_covLLH(df, config=CNVconfig)\n",
    "llh_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rolling window for log2ratio and llh\n",
    "+ compute the mean for log2ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:27:31.164092Z",
     "start_time": "2021-06-05T04:27:31.150823Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate(df, data_col, ref_col='FullExonPos', expand_limit=20):\n",
    "    '''\n",
    "    interpolates missing values in data_col using linear interpolation based on ref_col\n",
    "    '''\n",
    "    cols = list(df.columns)\n",
    "    # set FullExonPos as index for the interpolation method to work on proper intervals\n",
    "    df = df.reset_index(drop=False).set_index(ref_col, drop=False)\n",
    "    df.loc[:,data_col] = df[data_col].interpolate(method='values', limit=expand_limit, limit_direction='both')\n",
    "    return df.set_index('index')[cols]\n",
    "\n",
    "\n",
    "def normalize_df(df, col):\n",
    "    '''\n",
    "    normalize a column of a df\n",
    "    '''\n",
    "    _min = df[col].min()\n",
    "    _max = df[col].max()\n",
    "    df.loc[:, col] = (df[col] - _min) / (_max - _min)\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_col_rolling(df, col, aggr=\"mean\", window_size=200, roll_config={}):\n",
    "    \"\"\"\n",
    "    performs rolling computation of <agg> on data column <col> with given window size\n",
    "    the aggregation can be a:\n",
    "        - callable taking df[col] as argument and returning a scalar\n",
    "            column name will be taken from function name (stripping underscores)\n",
    "        - string expression understood by the agg-function of the pandas.groupby API\n",
    "            column name will be composed of col + aggr\n",
    "    computation is performed on a left and right rolling window\n",
    "    missing margins are filled by the counterpart window function\n",
    "    a diff column is included ()\n",
    "    \"\"\"\n",
    "    # UNPACK PARAMS\n",
    "    normalize = roll_config[\"normalize\"]\n",
    "    debug = roll_config[\"debug\"]\n",
    "    diff_exp = roll_config.get(\"diffexp\", 2)\n",
    "    ddof = roll_config.get(\"ddof\", 0)\n",
    "\n",
    "    # save org cols\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # ###### ROLLING LEFT\n",
    "    # check if aggr is a function\n",
    "    if callable(aggr):\n",
    "        if debug:\n",
    "            show_output(f\"Aggregating custom function {aggr.__name__}\")\n",
    "        df.loc[:, \"L\"] = df[col].rolling(window_size).apply(aggr)\n",
    "        # pass the function name for ensuing column naming\n",
    "        col_name = aggr.__name__\n",
    "    else:\n",
    "        # get the right computation by passing aggr to .agg()\n",
    "        # only this allows passing methods as string\n",
    "        df.loc[:, \"L\"] = df[col].rolling(window_size).agg(aggr, ddof=ddof)\n",
    "        col_name = col + \"_\"+  aggr\n",
    "\n",
    "    # ###### ROLLING RIGHT\n",
    "    # rolling right by shifting the L column\n",
    "    df.loc[:, \"R\"] = df.shift(-window_size + 1)[\"L\"]\n",
    "\n",
    "    # ###### DIFFING\n",
    "    diff_name = col_name + \"Diff\"\n",
    "\n",
    "    added_cols = [col_name, diff_name]\n",
    "    if debug:\n",
    "        added_cols += [f\"{col_name}L\", f\"{col_name}R\"]\n",
    "    # skips interpolation if value == 0\n",
    "    if interpolate:\n",
    "        # interpolate missing values\n",
    "        for c in [\"L\", \"R\"]:\n",
    "            df = interpolate(df, c, expand_limit=10)\n",
    "    # fill the margins\n",
    "    try:\n",
    "        L_margin = df[\"L\"].first_valid_index()\n",
    "        df.loc[:L_margin, \"L\"] = df[\"R\"]\n",
    "        R_margin = df[\"R\"].last_valid_index() + 1\n",
    "        df.loc[R_margin:, \"R\"] = df[\"L\"]\n",
    "    except Exception as e:\n",
    "        show_output(\n",
    "            f\"An error occurred attempting to fill the margins! {e}\", color=\"warning\"\n",
    "        )\n",
    "\n",
    "    # get the Diff\n",
    "    df.loc[:, diff_name] = np.abs(df[\"R\"] - df[\"L\"])\n",
    "    # normalize to max\n",
    "    df.loc[:, diff_name] = df[diff_name] / df[diff_name].max()\n",
    "    # here, contribution of L and R is controlled by diff value\n",
    "    df.loc[:, col_name] = df[\"R\"] * df[diff_name] + df[\"L\"] * (1 - df[diff_name])\n",
    "\n",
    "    if normalize:\n",
    "        df = normalize_df(df, col_name)\n",
    "        if debug:\n",
    "            for c in [\"L\", \"R\"]:\n",
    "                df = normalize(df, c)\n",
    "\n",
    "    # steepen the diff\n",
    "    df.loc[:, diff_name] = df[diff_name] ** diff_exp\n",
    "\n",
    "    if debug:\n",
    "        # specify col names of L and R\n",
    "        df = df.rename(columns=dict(L=f\"{col_name}L\", R=f\"{col_name}R\"))\n",
    "\n",
    "    # insert the cols\n",
    "    # get col index of cov_col for inserting log_col\n",
    "    insert_index = df.columns.get_loc(col) + 1\n",
    "    out_cols = cols[:insert_index] + added_cols + cols[insert_index:]\n",
    "\n",
    "    # reduce to the right columns\n",
    "    return df.loc[:, out_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T04:27:33.814961Z",
     "start_time": "2021-06-05T04:27:33.805947Z"
    }
   },
   "outputs": [],
   "source": [
    "def rolling_data(df, data_params={}, roll_config={}):\n",
    "    '''\n",
    "    cycles through the data params (rolling_data object from config dict)\n",
    "    and performs rolling computations for these params\n",
    "    '''\n",
    "    # now do global normalization for sum aggregations:\n",
    "    # cycle through the data_types\n",
    "    for data_type in data_params.keys():\n",
    "        # cycle through the cols for that data_type\n",
    "        for data_col in df.columns:\n",
    "            if not data_col.startswith(data_type):\n",
    "                continue\n",
    "            # cycle through the aggs for that data_type\n",
    "            for agg in data_params[data_type].keys():\n",
    "                # get params\n",
    "                window_size = data_params[data_type][agg]\n",
    "                show_output(f\"Computing rolling window for {agg} of {data_col} with window size {window_size}\")\n",
    "                # run the rolling window\n",
    "                df = one_col_rolling(\n",
    "                     df, \n",
    "                     col=data_col,\n",
    "                     aggr=agg,\n",
    "                     window_size=window_size,\n",
    "                     roll_config=roll_config\n",
    "                 )\n",
    "                \n",
    "            \n",
    "            #### Normalization\n",
    "            # only do normalization for sum aggregations\n",
    "            if not agg == \"sum\":\n",
    "                continue\n",
    "            show_output(f\"Normalizing {data_col} {agg}\")\n",
    "            # get the columns for normalization\n",
    "            col_name = data_col + \"_\" + agg\n",
    "            cols = [col_name]\n",
    "            if roll_config['debug']:\n",
    "                cols += [f'{col_name}L', f'{col_name}R']\n",
    "            for c in cols:\n",
    "                _min = df[c].min()\n",
    "                _max = df[c].max()\n",
    "                df.loc[:, c] = (df[c] - _min) / (_max - _min)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rolling_coverage(cov_df, config={}):\n",
    "    '''\n",
    "    cycle through the chroms and perform rolling window computations of data set in config\n",
    "    '''\n",
    "\n",
    "    # split the params dict for easier access\n",
    "    cc = config['rolling']['cov']\n",
    "    data_params = cc['data']\n",
    "    \n",
    "    # perform llh computation for coverage if needed\n",
    "    if \"covLLH\" in data_params:\n",
    "        cov_df = compute_covLLH(cov_df, config=config)\n",
    "        \n",
    "    cov_df = rolling_data(cov_df, data_params=data_params, roll_config=cc)\n",
    "                   \n",
    "    return cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:23:45.848717Z",
     "start_time": "2021-06-01T11:23:04.365156Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['L', 'R'], axis=1)\n",
    "CNVconfig = get_CNVconfig(\n",
    "    \"../config/config_CNV.yaml\", \n",
    "    local_config=path_config)\n",
    "\n",
    "roll_df = rolling_coverage(df, config=CNVconfig)\n",
    "roll_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:24:19.793902Z",
     "start_time": "2021-06-01T11:24:10.466478Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, _, _, _ = plot_cov(roll_df, plots=[log2,log2mean], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T11:29:59.409909Z",
     "start_time": "2021-06-01T11:29:54.702099Z"
    }
   },
   "outputs": [],
   "source": [
    "r1 = 'chr20:47Mb-57Mb'\n",
    "r2 = 'chr4:110Mb-140Mb'\n",
    "fig, _, _, _ = plot_cov(roll_df, plots=[log2,log2mean, llh, llhdiff], chroms='all', region=r1, **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T10:18:35.214337Z",
     "start_time": "2021-06-01T10:18:27.624874Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr8', 'chr9', 'chr20']\n",
    "\n",
    "fig, _, _, _ = plot_cov(roll_df, plots=[log2,log2mean], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEGACY CODE\n",
    "+ uses rolling of filter_df and interpolation into full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_col_rolling(df, df_filter, col, aggr, window_size=200, expand_limit=20, normalize=False, debug=False, diff_exp=2, ddof=0):\n",
    "    '''\n",
    "    performs rolling computation of <agg> on data column <col> with given window size\n",
    "    the aggregation can be a:\n",
    "        - callable taking df[col] as argument and returning a scalar\n",
    "            column name will be taken from function name (stripping underscores)\n",
    "        - string expression understood by the agg-function of the pandas.groupby API\n",
    "            column name will be composed of col + aggr\n",
    "    computation is performed on a left and right rolling window\n",
    "    missing margins are filled by the counterpart window function\n",
    "    a diff column is included ()\n",
    "    '''\n",
    "\n",
    "    org_cols = list(df.columns)\n",
    "    # rolling left\n",
    "    # check if aggr is a function\n",
    "    if callable(aggr):\n",
    "        if debug:\n",
    "            show_output(f'Aggregating custom function {aggr.__name__}')\n",
    "        df.loc[:, 'L'] = df_filter[col].rolling(window_size).apply(aggr)\n",
    "        # pass the function name for ensuing column naming\n",
    "        col_name = aggr.__name__.replace('_', '')\n",
    "    else:\n",
    "        # get the right computation by passing aggr to .agg()\n",
    "        # only this allows passing methods as string\n",
    "        df.loc[:, 'L'] = df_filter[col].rolling(window_size).agg(aggr, ddof=ddof)\n",
    "        col_name = col + aggr\n",
    "        \n",
    "    # rolling right by shifting the L column\n",
    "    df.loc[:, 'R'] = df.shift(-window_size + 1)['L']\n",
    "\n",
    "    diff_name = col_name + \"Diff\"\n",
    "    new_cols = org_cols + [col_name, diff_name]\n",
    "    if debug:\n",
    "        new_cols += [f'{col_name}L', f'{col_name}R']\n",
    "    # skips interpolation if value == 0\n",
    "    if interpolate:\n",
    "        # interpolate missing values\n",
    "        for c in ['L', 'R']:\n",
    "            df = interpolate(df, c, expand_limit=expand_limit)\n",
    "    # fill the margins\n",
    "    L_margin = df['L'].first_valid_index()\n",
    "    df.loc[:L_margin, 'L'] = df['R']\n",
    "    R_margin = df['R'].last_valid_index() + 1\n",
    "    df.loc[R_margin:, 'R'] = df['L']\n",
    "\n",
    "    # get the Diff\n",
    "    df.loc[:, diff_name] = np.abs(df['R'] - df['L'])\n",
    "    # normalize to max\n",
    "    df.loc[:, diff_name] = df[diff_name] / df[diff_name].max()\n",
    "    # here, contribution of L and R is controlled by diff value\n",
    "    df.loc[:, col_name] = df['R'] * \\\n",
    "        df[diff_name] + df['L'] * (1 - df[diff_name])\n",
    "    \n",
    "    if normalize:\n",
    "        df = normalize_df(df, col_name)\n",
    "        if debug:\n",
    "            for c in ['L', 'R']:\n",
    "                df = normalize(df, c)\n",
    "    \n",
    "    # square the diff\n",
    "    df.loc[:, diff_name] = df[diff_name] ** diff_exp\n",
    "\n",
    "    if debug:\n",
    "        # specify col names of L and R\n",
    "        df = df.rename(columns=dict(L=f'{col_name}L', R=f'{col_name}R'))\n",
    "\n",
    "    # reduce to the right columns\n",
    "    return df[new_cols]\n",
    "\n",
    "\n",
    "def rolling_data(df, filter_df, expand=0.25, ddof=0, debug=False, data_params={}):\n",
    "    '''\n",
    "    cycles through the data params (rolling_data object from config dict)\n",
    "    and performs rolling computations for these params\n",
    "    '''\n",
    "    # now do global normalization for sum aggregations:\n",
    "    # cycle through rolling_data\n",
    "    for data_col in data_params.keys():\n",
    "        for agg in data_params[data_col].keys():\n",
    "            # cycle through the chroms\n",
    "            chrom_dfs = []\n",
    "            for chrom in df['Chr'].unique():\n",
    "                # get the chrom_dfs\n",
    "                chrom_df = df.query('Chr == @chrom').sort_values('FullExonPos')\n",
    "                filter_chrom_df = filter_df.query('Chr == @chrom').sort_values('FullExonPos')\n",
    "                window_size = data_params[data_col][agg]\n",
    "                expand_limit = int(expand * window_size)\n",
    "                # show_output(f\"Computing rolling window for {agg} of {data_col} with window size {window_size} on {chrom}\")\n",
    "                chrom_df = one_col_rolling(chrom_df, filter_chrom_df, data_col, agg, window_size=window_size,\n",
    "                                           expand_limit=expand_limit, ddof=ddof)            \n",
    "                chrom_dfs.append(chrom_df)\n",
    "            # combine the chrom_dfs\n",
    "            df = pd.concat(chrom_dfs).sort_values('FullExonPos')\n",
    "            \n",
    "            #### Normalization\n",
    "            # only do normalization for sum aggregations\n",
    "            if not agg == \"sum\":\n",
    "                continue\n",
    "            show_output(f\"Normalizing {data_col} {agg}\")\n",
    "            # get the columns for normalization\n",
    "            col_name = data_col + agg\n",
    "            cols = [col_name]\n",
    "            if debug:\n",
    "                cols += [f'{col_name}L', f'{col_name}R']\n",
    "            for c in cols:\n",
    "                _min = df[c].min()\n",
    "                _max = df[c].max()\n",
    "                df.loc[:, c] = (df[c] - _min) / (_max - _min)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rolling_coverage(cov_df, config):\n",
    "    '''\n",
    "    cycle through the chroms and perform rolling window computations of data set in config\n",
    "    '''\n",
    "\n",
    "    # split the params dict for easier access\n",
    "    params = config[]['cov']\n",
    "    filter_params = params['filter']\n",
    "    data_params = params['rolling_data']\n",
    "    \n",
    "    # get the params for filtering\n",
    "    min_cov = filter_params['min_cov']\n",
    "    min_PON_cov = filter_params['min_PON_cov']\n",
    "    max_PON_std = filter_params['max_PON_std']\n",
    "    \n",
    "    cov_df = cov_df.sort_values('FullExonPos')\n",
    "    filter_df = cov_df.query(\n",
    "            'Coverage >= @min_cov and PONmeanCov >= @min_PON_cov and PONstd < @max_PON_std')\n",
    "    \n",
    "    cov_df = rolling_data(cov_df, filter_df, expand=params['expand'], ddof=config['ddof'], debug=config['debug'], data_params=data_params)\n",
    "                   \n",
    "    return cov_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

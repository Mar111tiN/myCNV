{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:44:35.216612Z",
     "start_time": "2020-11-10T08:44:35.164530Z"
    }
   },
   "outputs": [],
   "source": [
    "# HOME\n",
    "import os\n",
    "import scipy\n",
    "# get the code\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from codeCNV.plot import plot_snp, plot_genomic\n",
    "from codeCNV.rollingCNV import apply_rolling_coverage\n",
    "\n",
    "home = '/Users/mahtin'\n",
    "home = '/Users/martinscience'\n",
    "# standard paths\n",
    "testdata = os.path.join(home,\"Dropbox/Icke/Work/somVar/testdata\")\n",
    "static_path = os.path.join(home, \"Dropbox/Icke/Work/static\")\n",
    "cluster_path = os.path.join(home, \"mount\")\n",
    "tooldata = os.path.join(home, \"Dropbox/Icke/Work/somVar/tooldata\")\n",
    "\n",
    "# load the config\n",
    "# edit config directly in yaml file\n",
    "import yaml\n",
    "config_file = '../config/config_devel.yaml'\n",
    "def get_config(config_file):\n",
    "        with open(config_file) as file:\n",
    "        # The FullLoader parameter handles the conversion from YAML\n",
    "        # scalar values to Python the dictionary format\n",
    "            config = yaml.load(file, Loader=yaml.FullLoader)['CNV']['combine']\n",
    "        return config\n",
    "config = get_config(config_file)\n",
    "\n",
    "\n",
    "cnvdata = os.path.join(tooldata, \"myCNVdata\")\n",
    "output_path = os.path.join(cnvdata, \"output\")\n",
    "\n",
    "# the path to the input data\n",
    "cnv_path = os.path.join(cnvdata, \"cnv\")\n",
    "cnvPON_path = cnv_path = os.path.join(cnvdata, \"chromCov\")\n",
    "cnv_path = os.path.join(cluster_path, \"scratch/develop/PONcoverage/cnv\")\n",
    "cnvPON_path = os.path.join(cluster_path, \"scratch/develop/PONcoverage/chromCov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:42:55.769128Z",
     "start_time": "2020-11-10T08:42:53.661985Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"02_A\"\n",
    "cov_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.cov'), sep='\\t').query('log2ratio == log2ratio')\n",
    "fig_params = dict(\n",
    "    figsize=(50,8),\n",
    "    colormap='coolwarm_r',\n",
    "    color_chroms=True,\n",
    "    ylim=(-1.5,3)\n",
    ")\n",
    "log2 = dict(\n",
    "        title='log2ratio',\n",
    "        plot_type='scatter',   # ['line', 'scatter']\n",
    "        data='log2ratio',\n",
    "        plot_args=dict(\n",
    "            linewidth=0.3,\n",
    "            color='black',\n",
    "            s=0.2,\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "fig, ax, _, chrom_df = plot_genomic(cov_df.query('Coverage > 50 and PONmeanCov > 50 and PONstd < 100'), plots=[log2], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the code\n",
    "+ also get the snp_df for transferring FullExonPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:49:07.557840Z",
     "start_time": "2020-11-10T08:44:44.380496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample 01_A\n",
      "\u001b[1;30;1mComputing log-likelihood of log2ratio belonging to center gaussian [mean:-0.077, sigma:0.244]\u001b[0m\n",
      "\u001b[1;30;1mNormalizing covllh sum\u001b[0m\n",
      "Running sample 01_B\n",
      "\u001b[1;30;1mComputing log-likelihood of log2ratio belonging to center gaussian [mean:-0.014, sigma:0.212]\u001b[0m\n",
      "\u001b[1;30;1mNormalizing covllh sum\u001b[0m\n",
      "Running sample 02_A\n",
      "\u001b[1;30;1mComputing log-likelihood of log2ratio belonging to center gaussian [mean:-0.104, sigma:0.256]\u001b[0m\n",
      "\u001b[1;30;1mNormalizing covllh sum\u001b[0m\n",
      "Running sample 02_B\n",
      "\u001b[1;30;1mComputing log-likelihood of log2ratio belonging to center gaussian [mean:-0.046, sigma:0.228]\u001b[0m\n",
      "\u001b[1;30;1mNormalizing covllh sum\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for sample in ['01_A', '01_B', '02_A', '02_B']:\n",
    "    print(f'Running sample {sample}')\n",
    "    snp_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.snp'), sep='\\t')\n",
    "    cov_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.cov'), sep='\\t').query('log2ratio == log2ratio')\n",
    "    snpcov_df, rolling_cov_df = apply_rolling_coverage(snp_df, cov_df, config)\n",
    "    snpcov_df.to_csv(os.path.join(output_path, f'tmp/{sample}.snpcov.gz'), sep='\\t', index=False, compression=\"gzip\")\n",
    "    rolling_cov_df.to_csv(os.path.join(output_path, f'CNV/{sample}.roll.cov.gz'), sep='\\t', index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:14.029056Z",
     "start_time": "2020-11-10T08:43:01.942679Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"02_A\"\n",
    "cov_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.cov'), sep='\\t').query('log2ratio == log2ratio')\n",
    "snp_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.snp'), sep='\\t')\n",
    "snpcov_df, rolling_cov_df = apply_rolling_coverage(snp_df, cov_df, config)\n",
    "\n",
    "log2mean = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomean',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llh = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covllhsum',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llhdiff = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covllhsumDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "log2diff = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2L = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanL',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='white',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2R = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanR',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='black',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr8', 'chr9', 'chr20']\n",
    "\n",
    "_, _, _, _ = plot_genomic(rolling_cov_df, plots=[log2,log2mean, llh], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:27:16.431862Z",
     "start_time": "2020-11-10T07:27:15.337301Z"
    }
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(rolling_cov_df, plots=[log2,llhdiff, llh], chroms=chroms, region='chr5', **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assumption: log2ratio is normal-distributed around mean value:\n",
    "+ with sigma = .2 one can approximate the center masses\n",
    "+ log-likelihood should be far below average at CNV areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:25.482285Z",
     "start_time": "2020-11-10T08:43:24.970661Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "df = cov_df.copy()\n",
    "sigma = df.query('-0.5 < log2ratio < .5')['log2ratio'].std()* 1\n",
    "\n",
    "# get the mean of the center band\n",
    "mean = df.query('-0.5 < log2ratio < .5')['log2ratio'].mean()\n",
    "print(\"mean = \", mean)\n",
    "print(\"sigma = \", sigma)\n",
    "def llh(data, mean, sigma):\n",
    "    s = np.sqrt(2 * np.pi) * sigma\n",
    "    return np.exp((data - mean)**2 / (-2*(sigma**2))) / s\n",
    "\n",
    "\n",
    "r = np.linspace(-2,2,10000)\n",
    "\n",
    "fig1, ax = plt.subplots(figsize=(10,4))\n",
    "_ = ax.scatter(r, llh(r, mean, sigma), s=.5, alpha=0.5);\n",
    "_ = ax.scatter(df['log2ratio'], rnd.random(len(df.index))*4, s=.01, alpha=.1)\n",
    "_ = ax.set_xlim(-1.5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the llh for center mass for entire sample\n",
    "+ get the global mean\n",
    "+ compute loglikelihood\n",
    "+ rolling sum for chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom function for llh computation\n",
    "+ parameters sigma factor and mean range are taken from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:28.076980Z",
     "start_time": "2020-11-10T08:43:28.071151Z"
    }
   },
   "outputs": [],
   "source": [
    "def llh(data, mean, sigma):\n",
    "    s = np.sqrt(2 * np.pi) * sigma\n",
    "    return np.exp((data - mean)**2 / (-2*(sigma**2))) / s\n",
    "\n",
    "def compute_coverage_llh(df, config):\n",
    "    '''\n",
    "    computes the local log-likelihood of belonging to the center gaussian\n",
    "    '''\n",
    "    \n",
    "    # get config params\n",
    "    params = config['coverage']['llh']\n",
    "    \n",
    "    min_log2ratio, max_log2ratio = params['center_range']\n",
    "    # get the sigma and mean of the center band log2ratio\n",
    "    center_logs = df.query('@min_log2ratio < log2ratio < @max_log2ratio')['log2ratio']\n",
    "    sigma = center_logs.std() * params['sigma_factor']\n",
    "    mean = center_logs.mean()\n",
    "    print(f\"Computing log-likelihood of log2ratio belonging to center gaussian [mean:{round(mean, 3)}, sigma:{round(sigma,3)}]\")\n",
    "    df.loc[:, 'covllh'] = llh(df['log2ratio'], mean, sigma)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:29.748449Z",
     "start_time": "2020-11-10T08:43:29.596553Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute llh\n",
    "df = compute_coverage_llh(df, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rolling window for log2ratio and llh\n",
    "#### compute the mean for log2ratio\n",
    "+ this is all done on chromosome-basis\n",
    "+ the rolling should be performed on coverage data filtered for:\n",
    "    * minimal coverage\n",
    "    * minimal coverage in PON samples\n",
    "    * maximal std of PON coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test rolling_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:33.990038Z",
     "start_time": "2020-11-10T08:43:33.974448Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate(df, data_col, ref_col='FullExonPos', expand_limit=20):\n",
    "    '''\n",
    "    interpolates missing values in data_col using linear interpolation based on ref_col\n",
    "    '''\n",
    "    cols = list(df.columns)\n",
    "    # set FullExonPos as index for the interpolation method to work on proper intervals\n",
    "    df = df.reset_index(drop=False).set_index(ref_col, drop=False)\n",
    "    df.loc[:,data_col] = df[data_col].interpolate(method='values', limit=expand_limit, limit_direction='both')\n",
    "    return df.set_index('index')[cols]\n",
    "\n",
    "\n",
    "def one_col_rolling(df, df_filter, col, aggr, window_size=200, expand_limit=20, normalize=False, debug=False, diff_exp=2, ddof=0):\n",
    "    '''\n",
    "    performs rolling computation of <agg> on data column <col> with given window size\n",
    "    the aggregation has to be a string expression understood by the agg-function of the pandas.groupby API\n",
    "    computation is performed on a left and right rolling window\n",
    "    missing margins are filled by the counterpart window function\n",
    "    a diff column is included ()\n",
    "\n",
    "    '''\n",
    "\n",
    "    org_cols = list(df.columns)\n",
    "    # rolling left\n",
    "    # get the right computation by passing aggr to .agg()\n",
    "    # only this allows passing methods as string\n",
    "    df.loc[:, 'L'] = df_filter[col].rolling(window_size).agg(aggr, ddof=ddof)\n",
    "    # rolling right by shifting the L column\n",
    "    df.loc[:, 'R'] = df.shift(-window_size + 1)['L']\n",
    "\n",
    "    col_name = col + aggr\n",
    "    diff_name = col_name + \"Diff\"\n",
    "    new_cols = org_cols + [col_name, diff_name]\n",
    "    if debug:\n",
    "        new_cols += [f'{col_name}L', f'{col_name}R']\n",
    "    # skips interpolation if value == 0\n",
    "    if interpolate:\n",
    "        # interpolate missing values\n",
    "        for c in ['L', 'R']:\n",
    "            df = interpolate(df, c, expand_limit=expand_limit)\n",
    "    # fill the margins\n",
    "    L_margin = df['L'].first_valid_index()\n",
    "    df.loc[:L_margin, 'L'] = df['R']\n",
    "    R_margin = df['R'].last_valid_index() + 1\n",
    "    df.loc[R_margin:, 'R'] = df['L']\n",
    "\n",
    "\n",
    "\n",
    "    # get the Diff\n",
    "    df.loc[:, diff_name] = np.abs(df['R'] - df['L'])\n",
    "    # normalize to max\n",
    "    df.loc[:, diff_name] = df[diff_name] / df[diff_name].max()\n",
    "    # here, contribution of L and R is controlled by diff value\n",
    "    df.loc[:, col_name] = df['R'] * \\\n",
    "        df[diff_name] + df['L'] * (1 - df[diff_name])\n",
    "\n",
    "    # square the diff\n",
    "    df.loc[:, diff_name] = df[diff_name] ** diff_exp\n",
    "    \n",
    "    if debug:\n",
    "        # specify col names of L and R\n",
    "        df = df.rename(columns=dict(L=f'{col_name}L', R=f'{col_name}R'))\n",
    "        \n",
    "    # reduce to the right columns\n",
    "    return df[new_cols]\n",
    "\n",
    "\n",
    "def rolling_coverage(cov_df, config):\n",
    "    '''\n",
    "    cycle through the chroms and perform rolling window computations of data set in config\n",
    "    '''\n",
    "\n",
    "    # split the params dict for easier access\n",
    "    params = config['coverage']\n",
    "    filter_params = params['filter']\n",
    "    data_params = params['rolling_data']\n",
    "    debug=config['debug']\n",
    "    # get the params for filtering\n",
    "    min_cov = filter_params['min_cov']\n",
    "    min_PON_cov = filter_params['min_PON_cov']\n",
    "    max_PON_std = filter_params['max_PON_std']\n",
    "    chrom_dfs = []\n",
    "    for chrom in cov_df['Chr'].unique():\n",
    "        # restrict to chrom\n",
    "        chrom_df = cov_df.query('Chr == @chrom').sort_values('FullExonPos')\n",
    "        # filter df\n",
    "        filter_df = chrom_df.query(\n",
    "            'Coverage >= @min_cov and PONmeanCov >= @min_PON_cov and PONstd < @max_PON_std')\n",
    "        for data_col in data_params.keys():\n",
    "            for agg in data_params[data_col].keys():\n",
    "                window_size = data_params[data_col][agg]\n",
    "                expand_limit = int(params['expand'] * window_size)\n",
    "                # print(f\"Computing rolling window for {agg} of {data_col} with window size {window_size} on {chrom}\")\n",
    "                chrom_df = one_col_rolling(chrom_df, filter_df, data_col, agg, \n",
    "                window_size=window_size,\n",
    "                expand_limit=expand_limit, \n",
    "                normalize=params['normalize'],\n",
    "                diff_exp=config['diff_exp'],\n",
    "                debug=debug)\n",
    "        chrom_dfs.append(chrom_df)\n",
    "    df = pd.concat(chrom_dfs).sort_values('FullExonPos')\n",
    "    \n",
    "    # now do global normalization for sum aggregations:\n",
    "    # cycle through rolling_data\n",
    "    for data_col in data_params.keys():\n",
    "        for agg in data_params[data_col].keys():\n",
    "            # only do normalization for sum aggregations\n",
    "            if not agg == \"sum\":\n",
    "                continue\n",
    "            print(f\"Normalizing {data_col} {agg}\")\n",
    "            # get the columns for normalization\n",
    "            col_name = data_col + agg\n",
    "            cols = [col_name]\n",
    "            if debug:\n",
    "                cols += [f'{col_name}L', f'{col_name}R']\n",
    "             \n",
    "            for c in cols:\n",
    "                _min = df[c].min()\n",
    "                _max = df[c].max()\n",
    "                df.loc[:, c] = (df[c] - _min) / (_max - _min)                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from one_col_rolling\n",
    "\n",
    "    # normalize values\n",
    "    # should be only used for sum aggregations\n",
    "    if normalize and aggr == 'sum':\n",
    "        # normalize the data\n",
    "        # print('Normalizing data')\n",
    "        _min = df['L'].min()\n",
    "        _max = df['L'].max()\n",
    "        for c in ['L', 'R']:\n",
    "            df.loc[:, c] = (df[c] - _min) / (_max - _min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:39.644812Z",
     "start_time": "2020-11-10T08:43:36.117059Z"
    }
   },
   "outputs": [],
   "source": [
    "roll_df = rolling_coverage(df, config)\n",
    "roll_df[200:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:43:42.019903Z",
     "start_time": "2020-11-10T08:43:40.361280Z"
    }
   },
   "outputs": [],
   "source": [
    "log2mean = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomean',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llhsum = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covllhsum',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "log2diff = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2L = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanL',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='white',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2R = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanR',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='black',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr20']\n",
    "\n",
    "_, _, _, _ = plot_genomic(roll_df, plots=[log2,log2mean, llhsum], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T06:18:21.315424Z",
     "start_time": "2020-11-10T06:18:20.080232Z"
    }
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(roll_df, plots=[log2,log2mean, llh], chroms=chroms, region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T06:38:41.143901Z",
     "start_time": "2020-11-10T06:38:40.268743Z"
    }
   },
   "outputs": [],
   "source": [
    "plots = [\n",
    "    log2,\n",
    "    log2mean,\n",
    "    log2diff,\n",
    "    log2L,\n",
    "    log2R\n",
    "]\n",
    "\n",
    "_, _, _, _ = plot_genomic(roll_df, plots=plots, chroms=chroms, region='chr17', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T14:05:12.656695Z",
     "start_time": "2020-11-06T14:05:11.629802Z"
    }
   },
   "outputs": [],
   "source": [
    "plots = [\n",
    "    log2,\n",
    "    log2mean,\n",
    "    log2diff,\n",
    "    log2L,\n",
    "    log2R\n",
    "]\n",
    "fig_params.update({'ylim': (-1.5,0.8)})\n",
    "_, _, _, _ = plot_genomic(cov2_df, plots=plots, chroms=chroms, region=r1, **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge coverage data into SNP\n",
    "+ reduce to important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:44:08.509666Z",
     "start_time": "2020-11-10T08:44:08.501299Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_fullexonpon(merge_df):\n",
    "    chrom_dfs = []\n",
    "    for chrom in merge_df['Chr'].unique():\n",
    "        chrom_df = merge_df.query('Chr == @chrom')\n",
    "        chrom_df = interpolate(chrom_df, 'FullExonPos', ref_col='Pos', expand_limit=1000000)\n",
    "        chrom_dfs.append(chrom_df)\n",
    "    df = pd.concat(chrom_dfs).sort_values('FullExonPos')\n",
    "    df.loc[:, 'FullExonPos'] = df['FullExonPos'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def mergeSNPnCov(cov_df, snp_df):\n",
    "    \n",
    "    # reduce the data to important columns\n",
    "    # snp\n",
    "    snp_keep_cols = list(snp_df.columns)[:3] + ['Depth', 'EBscore', 'VAF']\n",
    "    snp_df = snp_df.loc[:, snp_keep_cols]\n",
    "    # cov\n",
    "    cov_keep_cols = list(cov_df.columns)[:4]\n",
    "    for data in ['log2ratio', 'covllh']:\n",
    "        cov_keep_cols += [col for col in cov_df.columns if data in col]\n",
    "        \n",
    "    cov_df = cov_df.loc[:, cov_keep_cols]\n",
    "    \n",
    "    # merge the data\n",
    "    merge_df = cov_df.merge(snp_df, on=list(snp_df.columns[:3]), how='outer')\n",
    "    \n",
    "    # interpolate FullExonPos\n",
    "    merge_df = interpolate_fullexonpon(merge_df)\n",
    "\n",
    "    # interpolate the data\n",
    "    for col in [col for col in merge_df.columns if 'log2ratio' in col or 'covllh' in col]:\n",
    "        merge_df = interpolate(merge_df, col, expand_limit=100)\n",
    "    # reduce to VAF values\n",
    "    snpcov_df = merge_df.query('VAF == VAF')\n",
    "    cov_df = cov_df.query('log2ratiomean == log2ratiomean')\n",
    "    return snpcov_df, cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:44:17.038883Z",
     "start_time": "2020-11-10T08:44:09.452212Z"
    }
   },
   "outputs": [],
   "source": [
    "snp_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.snp'), sep='\\t')\n",
    "merge_df, _ = mergeSNPnCov(roll_df, snp_df)\n",
    "merge_df.query('FullExonPos != FullExonPos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:00:08.833961Z",
     "start_time": "2020-11-10T07:00:08.816746Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T20:02:05.976237Z",
     "start_time": "2020-10-28T20:02:05.961936Z"
    }
   },
   "outputs": [],
   "source": [
    "# non-fitters are greatly reduced\n",
    "merge_df.query('log2ratiomean != log2ratiomean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:02:12.784201Z",
     "start_time": "2020-11-10T07:02:12.781065Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_rolling_coverage(snp_df, cov_df, config):\n",
    "    '''\n",
    "    master function for rolling coverage\n",
    "    '''\n",
    "    # reduce cov_df to valid data\n",
    "    cov_df = cov_df.query('log2ratio == log2ratio')\n",
    "    \n",
    "    # compute llh\n",
    "    cov_df = compute_coverage_llh(cov_df, config)\n",
    "    \n",
    "    cov_df = rolling_coverage(cov_df, config)\n",
    "    \n",
    "    snpcov_df, rolling_cov_df = mergeSNPnCov(cov_df, snp_df)\n",
    "    \n",
    "    return snpcov_df, rolling_cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:04:23.182971Z",
     "start_time": "2020-11-10T07:04:11.396830Z"
    }
   },
   "outputs": [],
   "source": [
    "snpcov_df, rolling_cov_df = apply_rolling_coverage(snp_df, cov_df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:04:27.708611Z",
     "start_time": "2020-11-10T07:04:27.689714Z"
    }
   },
   "outputs": [],
   "source": [
    "snpcov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:04:29.232339Z",
     "start_time": "2020-11-10T07:04:29.216354Z"
    }
   },
   "outputs": [],
   "source": [
    "rolling_cov_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T15:58:02.113180Z",
     "start_time": "2020-12-15T15:58:02.070753Z"
    }
   },
   "outputs": [],
   "source": [
    "# HOME\n",
    "home = '/Users/mahtin'\n",
    "home = '/Users/martinscience'\n",
    "import os\n",
    "import scipy\n",
    "# get the code\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from codeCNV.plot import plot_snp, plot_genomic\n",
    "from script_utils import show_output\n",
    "from codeCNV.rollingCov import apply_rolling_coverage\n",
    "\n",
    "# standard paths\n",
    "testdata = os.path.join(home,\"Dropbox/Icke/Work/somVar/testdata\")\n",
    "static_path = os.path.join(home, \"Dropbox/Icke/Work/static\")\n",
    "cluster_path = os.path.join(home, \"mount\")\n",
    "tooldata = os.path.join(home, \"Dropbox/Icke/Work/somVar/tooldata\")\n",
    "\n",
    "# load the config\n",
    "# edit config directly in yaml file\n",
    "import yaml\n",
    "config_file = '../config/config_devel.yaml'\n",
    "def get_config(config_file, param):\n",
    "        with open(config_file) as file:\n",
    "        # The FullLoader parameter handles the conversion from YAML\n",
    "        # scalar values to Python the dictionary format\n",
    "            config = yaml.load(file, Loader=yaml.FullLoader)['CNV'][param]\n",
    "        return config\n",
    "config = get_config(config_file, 'combine')\n",
    "\n",
    "cnvdata = os.path.join(tooldata, \"myCNVdata\")\n",
    "output_path = os.path.join(cnvdata, \"output\")\n",
    "plot_path = f'{home}/Dropbox/Icke/Work/myLabmeeting/figures/matplotlib'\n",
    "\n",
    "plot_path = \"/Users/martinscience/Desktop/CNVAML/new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T16:00:41.962798Z",
     "start_time": "2020-12-15T16:00:39.930045Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"08_A\"\n",
    "cov_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.cov'), sep='\\t').query('log2ratio == log2ratio')\n",
    "fig_params = dict(\n",
    "    figsize=(25,4),\n",
    "    colormap='coolwarm_r',\n",
    "    color_chroms=True,\n",
    "    ylim=(-1.7,3)\n",
    ")\n",
    "log2 = dict(\n",
    "        title='log2ratio',\n",
    "        plot_type='scatter',   # ['line', 'scatter']\n",
    "        data='log2ratio',\n",
    "        plot_args=dict(\n",
    "            linewidth=0.3,\n",
    "            color='black',\n",
    "            s=.5,\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "fig, ax, _, chrom_df = plot_genomic(cov_df.query('Coverage > 30 and PONmeanCov > 50 and PONstd < 100'), plots=[log2], chroms='all', region='', **fig_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T16:01:12.220460Z",
     "start_time": "2020-12-15T16:01:11.384495Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax, _, chrom_df = plot_genomic(cov_df.query('Coverage > 30 and PONmeanCov > 50 and PONstd < 100'), plots=[log2], chroms='chr7', region='chr7:120Mb-130Mb', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T16:01:13.606391Z",
     "start_time": "2020-12-15T16:01:13.488538Z"
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig(f'{plot_path}/{sample}.cov.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the code\n",
    "+ also get the snp_df for transferring FullExonPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-16T20:31:45.293Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "samples = [str(a+1).zfill(2) for a in range(15)]\n",
    "\n",
    "for sample in [f\"{s}_{t}\" for s in samples for t in ['A', 'B']]:\n",
    "    snp_file = os.path.join(output_path, f'CNV/{sample}.snp')\n",
    "    cov_file = os.path.join(output_path, f'CNV/{sample}.cov')\n",
    "    if os.path.isfile(snp_file) and os.path.isfile(cov_file):\n",
    "        show_output(f'Running sample {sample}', time=True)\n",
    "        snp_df = pd.read_csv(snp_file, sep='\\t')\n",
    "        cov_df = pd.read_csv(cov_file, sep='\\t').query('log2ratio == log2ratio')\n",
    "        snpcov_df, rolling_cov_df = apply_rolling_coverage(snp_df, cov_df, config)\n",
    "        snpcov_df.to_csv(os.path.join(output_path, f'tmp/{sample}.snpcov.gz'), sep='\\t', index=False, compression=\"gzip\")\n",
    "        rolling_cov_df.to_csv(os.path.join(output_path, f'CNV/{sample}.roll.cov.gz'), sep='\\t', index=False, compression=\"gzip\")\n",
    "        show_output(f'Finished sample {sample}', time=True, color=\"success\")\n",
    "    else:\n",
    "        show_output(f\"Cannot find {sample}!\", color='warning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:42:29.105465Z",
     "start_time": "2020-12-15T14:42:10.577719Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"02_A\"\n",
    "cov_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.cov'), sep='\\t').query('log2ratio == log2ratio')\n",
    "snp_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.snp'), sep='\\t')\n",
    "config = get_config(config_file, 'combine')\n",
    "snpcov_df, rolling_cov_df = apply_rolling_coverage(snp_df, cov_df, config)\n",
    "\n",
    "log2mean = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomean',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llh = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covLLHsum',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llhdiff = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covLLHsumDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "log2diff = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2L = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanL',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='white',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2R = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanR',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='black',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr8', 'chr9', 'chr20']\n",
    "\n",
    "fig, _, _, _ = plot_genomic(rolling_cov_df, plots=[log2,log2mean], chroms='all', region='chr1', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:55:37.025524Z",
     "start_time": "2020-12-15T14:55:36.848536Z"
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig(f'{plot_path}/{sample}.chr3.covroll.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:55:20.264562Z",
     "start_time": "2020-12-15T14:55:18.963391Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, _, _, _ = plot_genomic(rolling_cov_df, plots=[log2,llhdiff, llh, ], chroms=chroms, region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:53:12.313210Z",
     "start_time": "2020-11-16T20:53:11.042877Z"
    }
   },
   "outputs": [],
   "source": [
    "log2 = dict(\n",
    "        title='log2ratio',\n",
    "        plot_type='scatter',   # ['line', 'scatter']\n",
    "        data='log2ratio',\n",
    "        plot_args=dict(\n",
    "            linewidth=0.3,\n",
    "            color='black',\n",
    "            s=.5,\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "_, _, _, _ = plot_genomic(rolling_cov_df, plots=[log2,llhdiff, llh], chroms=chroms, region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:53:00.061876Z",
     "start_time": "2020-11-16T20:52:59.465812Z"
    }
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(rolling_cov_df.query('covCNVcore > 0'), plots=[log2,llhdiff, llh], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:53:02.851036Z",
     "start_time": "2020-11-16T20:53:02.270024Z"
    }
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(rolling_cov_df.query('covCNVcore > 0'), plots=[log2,llhdiff, llh], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assumption: log2ratio is normal-distributed around mean value:\n",
    "+ with sigma = .2 one can approximate the center masses\n",
    "+ log-likelihood should be far below average at CNV areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T13:29:17.263614Z",
     "start_time": "2020-12-15T13:29:16.750022Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "df = cov_df.copy()\n",
    "sigma = df.query('-0.5 < log2ratio < .5')['log2ratio'].std()* 1\n",
    "\n",
    "# get the mean of the center band\n",
    "mean = df.query('-0.5 < log2ratio < .5')['log2ratio'].mean()\n",
    "print(\"mean = \", mean)\n",
    "print(\"sigma = \", sigma)\n",
    "def llh(data, mean, sigma):\n",
    "    s = np.sqrt(2 * np.pi) * sigma\n",
    "    return np.exp((data - mean)**2 / (-2*(sigma**2))) / s\n",
    "\n",
    "\n",
    "r = np.linspace(-2,2,10000)\n",
    "\n",
    "fig1, ax = plt.subplots(figsize=(10,4))\n",
    "_ = ax.scatter(r, llh(r, mean, sigma), s=.5, alpha=0.5);\n",
    "_ = ax.scatter(df['log2ratio'], rnd.random(len(df.index))*4, s=.01, alpha=.1)\n",
    "_ = ax.set_xlim(-1.5,2)\n",
    "\n",
    "# fig1.savefig(f'{plot_path}/{sample}.log2gauss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the llh for center mass for entire sample\n",
    "+ get the global mean\n",
    "+ compute loglikelihood\n",
    "+ rolling sum for chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom function for llh computation\n",
    "+ parameters sigma factor and mean range are taken from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T13:32:49.866806Z",
     "start_time": "2020-12-15T13:32:49.861412Z"
    }
   },
   "outputs": [],
   "source": [
    "def llh(data, mean, sigma):\n",
    "    s = np.sqrt(2 * np.pi) * sigma\n",
    "    return np.exp((data - mean)**2 / (-2*(sigma**2))) / s\n",
    "\n",
    "def compute_coverage_llh(df, config):\n",
    "    '''\n",
    "    computes the local log-likelihood of belonging to the center gaussian\n",
    "    '''\n",
    "    \n",
    "    # get config params\n",
    "    params = config['cov']['LLH']\n",
    "    \n",
    "    min_log2ratio, max_log2ratio = params['center_range']\n",
    "    # get the sigma and mean of the center band log2ratio\n",
    "    center_logs = df.query('@min_log2ratio < log2ratio < @max_log2ratio')['log2ratio']\n",
    "    sigma = center_logs.std() * params['sigma_factor']\n",
    "    mean = center_logs.mean()\n",
    "    print(f\"Computing log-likelihood of log2ratio belonging to center gaussian [mean:{round(mean, 3)}, sigma:{round(sigma,3)}]\")\n",
    "    df.loc[:, 'covLLH'] = llh(df['log2ratio'], mean, sigma)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T13:32:51.909461Z",
     "start_time": "2020-12-15T13:32:51.728398Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute llh\n",
    "df = compute_coverage_llh(df, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rolling window for log2ratio and llh\n",
    "#### compute the mean for log2ratio\n",
    "+ this is all done on chromosome-basis\n",
    "+ the rolling should be performed on coverage data filtered for:\n",
    "    * minimal coverage\n",
    "    * minimal coverage in PON samples\n",
    "    * maximal std of PON coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test rolling_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T15:00:59.390454Z",
     "start_time": "2020-12-15T15:00:59.371071Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate(df, data_col, ref_col='FullExonPos', expand_limit=20):\n",
    "    '''\n",
    "    interpolates missing values in data_col using linear interpolation based on ref_col\n",
    "    '''\n",
    "    cols = list(df.columns)\n",
    "    # set FullExonPos as index for the interpolation method to work on proper intervals\n",
    "    df = df.reset_index(drop=False).set_index(ref_col, drop=False)\n",
    "    df.loc[:,data_col] = df[data_col].interpolate(method='values', limit=expand_limit, limit_direction='both')\n",
    "    return df.set_index('index')[cols]\n",
    "\n",
    "\n",
    "def normalize_df(df, col):\n",
    "    '''\n",
    "    normalize a column of a df\n",
    "    '''\n",
    "    _min = df[col].min()\n",
    "    _max = df[col].max()\n",
    "    df.loc[:, col] = (df[col] - _min) / (_max - _min)\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_col_rolling(df, df_filter, col, aggr, window_size=200, expand_limit=20, normalize=False, debug=False, diff_exp=2, ddof=0):\n",
    "    '''\n",
    "    performs rolling computation of <agg> on data column <col> with given window size\n",
    "    the aggregation can be a:\n",
    "        - callable taking df[col] as argument and returning a scalar\n",
    "            column name will be taken from function name (stripping underscores)\n",
    "        - string expression understood by the agg-function of the pandas.groupby API\n",
    "            column name will be composed of col + aggr\n",
    "    computation is performed on a left and right rolling window\n",
    "    missing margins are filled by the counterpart window function\n",
    "    a diff column is included ()\n",
    "    '''\n",
    "\n",
    "    org_cols = list(df.columns)\n",
    "    # rolling left\n",
    "    # check if aggr is a function\n",
    "    if callable(aggr):\n",
    "        if debug:\n",
    "            show_output(f'Aggregating custom function {aggr.__name__}')\n",
    "        df.loc[:, 'L'] = df_filter[col].rolling(window_size).apply(aggr)\n",
    "        # pass the function name for ensuing column naming\n",
    "        col_name = aggr.__name__.replace('_', '')\n",
    "    else:\n",
    "        # get the right computation by passing aggr to .agg()\n",
    "        # only this allows passing methods as string\n",
    "        df.loc[:, 'L'] = df_filter[col].rolling(window_size).agg(aggr, ddof=ddof)\n",
    "        col_name = col + aggr\n",
    "        \n",
    "    # rolling right by shifting the L column\n",
    "    df.loc[:, 'R'] = df.shift(-window_size + 1)['L']\n",
    "\n",
    "    diff_name = col_name + \"Diff\"\n",
    "    new_cols = org_cols + [col_name, diff_name]\n",
    "    if debug:\n",
    "        new_cols += [f'{col_name}L', f'{col_name}R']\n",
    "    # skips interpolation if value == 0\n",
    "    if interpolate:\n",
    "        # interpolate missing values\n",
    "        for c in ['L', 'R']:\n",
    "            df = interpolate(df, c, expand_limit=expand_limit)\n",
    "    # fill the margins\n",
    "    L_margin = df['L'].first_valid_index()\n",
    "    df.loc[:L_margin, 'L'] = df['R']\n",
    "    R_margin = df['R'].last_valid_index() + 1\n",
    "    df.loc[R_margin:, 'R'] = df['L']\n",
    "\n",
    "    # get the Diff\n",
    "    df.loc[:, diff_name] = np.abs(df['R'] - df['L'])\n",
    "    # normalize to max\n",
    "    df.loc[:, diff_name] = df[diff_name] / df[diff_name].max()\n",
    "    # here, contribution of L and R is controlled by diff value\n",
    "    df.loc[:, col_name] = df['R'] * \\\n",
    "        df[diff_name] + df['L'] * (1 - df[diff_name])\n",
    "    \n",
    "    if normalize:\n",
    "        df = normalize_df(df, col_name)\n",
    "        if debug:\n",
    "            for c in ['L', 'R']:\n",
    "                df = normalize(df, c)\n",
    "    \n",
    "    # square the diff\n",
    "    df.loc[:, diff_name] = df[diff_name] ** diff_exp\n",
    "\n",
    "    if debug:\n",
    "        # specify col names of L and R\n",
    "        df = df.rename(columns=dict(L=f'{col_name}L', R=f'{col_name}R'))\n",
    "\n",
    "    # reduce to the right columns\n",
    "    return df[new_cols]\n",
    "\n",
    "\n",
    "def rolling_data(df, filter_df, expand=0.25, ddof=0, debug=False, data_params={}):\n",
    "    '''\n",
    "    cycles through the data params (rolling_data object from config dict)\n",
    "    and performs rolling computations for these params\n",
    "    '''\n",
    "    # now do global normalization for sum aggregations:\n",
    "    # cycle through rolling_data\n",
    "    for data_col in data_params.keys():\n",
    "        for agg in data_params[data_col].keys():\n",
    "            # cycle through the chroms\n",
    "            chrom_dfs = []\n",
    "            for chrom in df['Chr'].unique():\n",
    "                # get the chrom_dfs\n",
    "                chrom_df = df.query('Chr == @chrom').sort_values('FullExonPos')\n",
    "                filter_chrom_df = filter_df.query('Chr == @chrom').sort_values('FullExonPos')\n",
    "                window_size = data_params[data_col][agg]\n",
    "                expand_limit = int(expand * window_size)\n",
    "                # show_output(f\"Computing rolling window for {agg} of {data_col} with window size {window_size} on {chrom}\")\n",
    "                chrom_df = one_col_rolling(chrom_df, filter_chrom_df, data_col, agg, window_size=window_size,\n",
    "                                           expand_limit=expand_limit, ddof=ddof)            \n",
    "                chrom_dfs.append(chrom_df)\n",
    "            # combine the chrom_dfs\n",
    "            df = pd.concat(chrom_dfs).sort_values('FullExonPos')\n",
    "            \n",
    "            #### Normalization\n",
    "            # only do normalization for sum aggregations\n",
    "            if not agg == \"sum\":\n",
    "                continue\n",
    "            print(f\"Normalizing {data_col} {agg}\")\n",
    "            # get the columns for normalization\n",
    "            col_name = data_col + agg\n",
    "            cols = [col_name]\n",
    "            if debug:\n",
    "                cols += [f'{col_name}L', f'{col_name}R']\n",
    "            for c in cols:\n",
    "                _min = df[c].min()\n",
    "                _max = df[c].max()\n",
    "                df.loc[:, c] = (df[c] - _min) / (_max - _min)\n",
    "    return df\n",
    "\n",
    "def rolling_coverage(cov_df, config):\n",
    "    '''\n",
    "    cycle through the chroms and perform rolling window computations of data set in config\n",
    "    '''\n",
    "\n",
    "    # split the params dict for easier access\n",
    "    params = config['cov']\n",
    "    filter_params = params['filter']\n",
    "    data_params = params['rolling_data']\n",
    "    \n",
    "    # get the params for filtering\n",
    "    min_cov = filter_params['min_cov']\n",
    "    min_PON_cov = filter_params['min_PON_cov']\n",
    "    max_PON_std = filter_params['max_PON_std']\n",
    "    \n",
    "    cov_df = cov_df.sort_values('FullExonPos')\n",
    "    filter_df = cov_df.query(\n",
    "            'Coverage >= @min_cov and PONmeanCov >= @min_PON_cov and PONstd < @max_PON_std')\n",
    "    \n",
    "    cov_df = rolling_data(cov_df, filter_df, expand=params['expand'], ddof=config['ddof'], debug=config['debug'], data_params=data_params)\n",
    "                   \n",
    "    return cov_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from one_col_rolling\n",
    "\n",
    "    # normalize values\n",
    "    # should be only used for sum aggregations\n",
    "    if normalize and aggr == 'sum':\n",
    "        # normalize the data\n",
    "        # print('Normalizing data')\n",
    "        _min = df['L'].min()\n",
    "        _max = df['L'].max()\n",
    "        for c in ['L', 'R']:\n",
    "            df.loc[:, c] = (df[c] - _min) / (_max - _min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T15:01:07.733851Z",
     "start_time": "2020-12-15T15:01:02.057486Z"
    }
   },
   "outputs": [],
   "source": [
    "roll_df = rolling_coverage(df, config)\n",
    "roll_df[200:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T15:01:29.934513Z",
     "start_time": "2020-12-15T15:01:28.245433Z"
    }
   },
   "outputs": [],
   "source": [
    "log2mean = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomean',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "llhsum = dict(\n",
    "        title='llh',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='covLLHsum',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "log2diff = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanDiff',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='blue',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2L = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanL',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='white',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "log2R = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomeanR',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='black',\n",
    "            alpha=.7\n",
    "        ))\n",
    "\n",
    "chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr20']\n",
    "\n",
    "_, _, _, _ = plot_genomic(roll_df, plots=[log2,log2mean, llhsum], chroms='all', region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T15:01:31.252549Z",
     "start_time": "2020-12-15T15:01:30.141764Z"
    }
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(roll_df, plots=[log2,log2mean, llh], chroms=chroms, region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T06:38:41.143901Z",
     "start_time": "2020-11-10T06:38:40.268743Z"
    }
   },
   "outputs": [],
   "source": [
    "plots = [\n",
    "    log2,\n",
    "    log2mean,\n",
    "    log2diff,\n",
    "    log2L,\n",
    "    log2R\n",
    "]\n",
    "\n",
    "_, _, _, _ = plot_genomic(roll_df, plots=plots, chroms=chroms, region='chr17', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T14:05:12.656695Z",
     "start_time": "2020-11-06T14:05:11.629802Z"
    }
   },
   "outputs": [],
   "source": [
    "plots = [\n",
    "    log2,\n",
    "    log2mean,\n",
    "    log2diff,\n",
    "    log2L,\n",
    "    log2R\n",
    "]\n",
    "fig_params.update({'ylim': (-1.5,0.8)})\n",
    "_, _, _, _ = plot_genomic(cov2_df, plots=plots, chroms=chroms, region=r1, **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract blocks of CNV for clustering\n",
    "+ needs to be done before the fusing with SNP in order to make use of high definition diff\n",
    "+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocks(df, col, min_size=0):\n",
    "    '''\n",
    "    takes a column of binary containment to certain group and returns block numbers and respective block_sizes\n",
    "    excludes blocks below a certain block size limit\n",
    "    '''\n",
    "    \n",
    "    org_cols = list(df.columns)\n",
    "    # find gaps where col value changes\n",
    "    df.loc[:, ['gap']] = (df[col] != df.shift(1)[col]).astype(int)\n",
    "    # set the col names\n",
    "    blocksize = f'{col}block_size'\n",
    "    block = f'{col}block'\n",
    "    \n",
    "    # enumerate the gaps for blockID where value is 1\n",
    "    df.loc[:, [block]] = df['gap'].cumsum() * df[col]\n",
    "    # group by blocks and count size\n",
    "    blocks = df.groupby(block)['gap'].count().rename(blocksize)\n",
    "    # merge block size into df\n",
    "    df = df.merge(blocks, left_on=block, right_index=True)\n",
    "    # remove miniscule blocks\n",
    "    df.loc[df[blocksize] < min_size, block] = 0\n",
    "    \n",
    "    ### maybe adjust block labelling to be numerically ordered\n",
    "    # should maybe done \n",
    "    df.loc[:, col]  = df[block]\n",
    "    # cols = org_cols + [block, blocksize]\n",
    "    return df[org_cols]\n",
    "\n",
    "\n",
    "def get_CNV_blocks(df, data, config):\n",
    "    '''\n",
    "    finds blocks of CNV with center LLH below threshold\n",
    "    then it reduces these blocks to the regions within the Diff-peaks.. \n",
    "    ..to only enter the most meaningful data into clustering\n",
    "\n",
    "    do the same thing for covCenter with values above center threshold\n",
    "    (definitely belonging to the center)\n",
    "    '''\n",
    "\n",
    "    # extract params from config\n",
    "    # for covLLH --> lookup combine.cov.LLH_cutoff t=cov\n",
    "    t = data.replace('LLH', \"\")\n",
    "    params = config[t]\n",
    "\n",
    "    LLH_params = params['LLH_cutoff']\n",
    "\n",
    "    col = data + \"sum\"\n",
    "    diff_col = data + \"Diff\"\n",
    "\n",
    "    # get the covCNV\n",
    "\n",
    "    # get boolint whether LLH falls below threshold\n",
    "    # fillna(1) to exclude any missing coverages\n",
    "    df.loc[:, f'{t}CNV'] = (df[col].fillna(1) < LLH_params['cnv']).astype(int)\n",
    "    # get the covCNV blocks\n",
    "    df = get_blocks(df, f'{t}CNV', min_size=LLH_params['min_block_size'])\n",
    "    # reduce data to within Diff-peaks\n",
    "    df.loc[:, f'{t}CNVcore'] = ((df[f'{t}CNV'] > 0) & (\n",
    "        df[f'{t}LLHsumDiff'] < LLH_params['max_diff'])).astype(int)\n",
    "\n",
    "    # here I could also expand the covCNV to the peak of the Diff for covCNVexp\n",
    "    # get the window_size for core expanding\n",
    "    window_size = params['rolling_data'][data]['sum']\n",
    "\n",
    "    # get the covCenter\n",
    "    df.loc[:, f'{t}Center'] = (df[col].fillna(\n",
    "        0) > LLH_params['center']).astype(int)\n",
    "    # get the covCenter blocks\n",
    "    df = get_blocks(df, f'{t}Center', min_size=LLH_params['min_block_size'])\n",
    "    # reduce data to within Diff-peaks\n",
    "    df.loc[:, f'{t}Centercore'] = ((df[f'{t}Center'] > 0) & (\n",
    "        df[f'{t}LLHsumDiff'] < LLH_params['max_diff'])).astype(int)\n",
    "    # get the window_size for core expanding\n",
    "    window_size = params['rolling_data'][data]['sum']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_CNV_blocks(rolling_cov_df, 'covLLH', config)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(df.query('covCentercore > 0'), plots=[log2,llhdiff, llh], chroms=chroms, region='', **fig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _ = plot_genomic(df.query('covCNVcore > 0'), plots=[log2,llhdiff, llh], chroms=chroms, region='', **fig_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge coverage data into SNP\n",
    "+ reduce to important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:44:08.509666Z",
     "start_time": "2020-11-10T08:44:08.501299Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_fullexonpon(merge_df):\n",
    "    chrom_dfs = []\n",
    "    for chrom in merge_df['Chr'].unique():\n",
    "        chrom_df = merge_df.query('Chr == @chrom')\n",
    "        chrom_df = interpolate(chrom_df, 'FullExonPos', ref_col='Pos', expand_limit=1000000)\n",
    "        chrom_dfs.append(chrom_df)\n",
    "    df = pd.concat(chrom_dfs).sort_values('FullExonPos')\n",
    "    df.loc[:, 'FullExonPos'] = df['FullExonPos'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def mergeSNPnCov(cov_df, snp_df):\n",
    "    \n",
    "    # reduce the data to important columns\n",
    "    # snp\n",
    "    snp_keep_cols = list(snp_df.columns)[:3] + ['Depth', 'EBscore', 'VAF']\n",
    "    snp_df = snp_df.loc[:, snp_keep_cols]\n",
    "    # cov\n",
    "    cov_keep_cols = list(cov_df.columns)[:4]\n",
    "    for data in ['log2ratio', 'covLLH', 'covC']:\n",
    "        cov_keep_cols += [col for col in cov_df.columns if data in col]\n",
    "        \n",
    "    cov_df = cov_df.loc[:, cov_keep_cols]\n",
    "    \n",
    "    # merge the data\n",
    "    merge_df = cov_df.merge(snp_df, on=list(snp_df.columns[:3]), how='outer')\n",
    "    \n",
    "    # interpolate FullExonPos\n",
    "    merge_df = interpolate_fullexonpon(merge_df)\n",
    "\n",
    "    # interpolate the data\n",
    "    for col in [col for col in merge_df.columns if 'log2ratio' in col or 'covLLH' in col or 'covC' in col]:\n",
    "        merge_df = interpolate(merge_df, col, expand_limit=100)\n",
    "    # reduce to VAF values\n",
    "    snpcov_df = merge_df.query('VAF == VAF')\n",
    "    cov_df = cov_df.query('log2ratiomean == log2ratiomean')\n",
    "    return snpcov_df, cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T08:44:17.038883Z",
     "start_time": "2020-11-10T08:44:09.452212Z"
    }
   },
   "outputs": [],
   "source": [
    "snp_df = pd.read_csv(os.path.join(output_path, f'CNV/{sample}.snp'), sep='\\t')\n",
    "merge_df, _ = mergeSNPnCov(roll_df, snp_df)\n",
    "merge_df.query('FullExonPos != FullExonPos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:00:08.833961Z",
     "start_time": "2020-11-10T07:00:08.816746Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T20:02:05.976237Z",
     "start_time": "2020-10-28T20:02:05.961936Z"
    }
   },
   "outputs": [],
   "source": [
    "# non-fitters are greatly reduced\n",
    "merge_df.query('log2ratiomean != log2ratiomean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:02:12.784201Z",
     "start_time": "2020-11-10T07:02:12.781065Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_rolling_coverage(snp_df, cov_df, config):\n",
    "    '''\n",
    "    master function for rolling coverage\n",
    "    '''\n",
    "    # reduce cov_df to valid data\n",
    "    cov_df = cov_df.query('log2ratio == log2ratio')\n",
    "\n",
    "    # compute llh\n",
    "    show_output(\n",
    "        f\"Computing covCenter log-likelihood.\")\n",
    "    cov_df = compute_coverage_llh(cov_df, config)\n",
    "    # compute llh\n",
    "    show_output(\n",
    "        f\"Performing rolling coverage.\")\n",
    "    cov_df = rolling_coverage(cov_df, config)\n",
    "    show_output(\n",
    "        f\"Identifying CNV blocks.\")\n",
    "    cov_df = get_CNV_blocks(cov_df, 'covLLH', config)\n",
    "    snpcov_df, rolling_cov_df = mergeSNPnCov(cov_df, snp_df)\n",
    "    return snpcov_df, rolling_cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:04:23.182971Z",
     "start_time": "2020-11-10T07:04:11.396830Z"
    }
   },
   "outputs": [],
   "source": [
    "snpcov_df, rolling_cov_df = apply_rolling_coverage(snp_df, cov_df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:04:27.708611Z",
     "start_time": "2020-11-10T07:04:27.689714Z"
    }
   },
   "outputs": [],
   "source": [
    "snpcov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:04:29.232339Z",
     "start_time": "2020-11-10T07:04:29.216354Z"
    }
   },
   "outputs": [],
   "source": [
    "rolling_cov_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

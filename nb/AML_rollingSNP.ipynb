{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make plots for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T05:42:45.197566Z",
     "start_time": "2020-12-16T05:42:45.024001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMLPresi.pptx         \u001b[34mP559\u001b[m\u001b[m                  SNPplots.ipynb\r\n",
      "AML_relapse.csv       \u001b[34mP615\u001b[m\u001b[m                  \u001b[34mSampleCheck\u001b[m\u001b[m\r\n",
      "AML_relapse.xlsx      \u001b[34mP625\u001b[m\u001b[m                  WES CNV Analysis.xlsx\r\n",
      "AML_relapse1.xlsx     \u001b[34mP665\u001b[m\u001b[m                  filter1.csv\r\n",
      "\u001b[34mCNVplots\u001b[m\u001b[m              \u001b[34mP685\u001b[m\u001b[m                  filter1GDC.csv\r\n",
      "\u001b[34mCNVraw\u001b[m\u001b[m                \u001b[34mP778\u001b[m\u001b[m                  samples.csv\r\n",
      "\u001b[34mP12\u001b[m\u001b[m                   \u001b[34mPGDC\u001b[m\u001b[m\r\n",
      "\u001b[34mP483\u001b[m\u001b[m                  \u001b[34mPmerge\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "######## need to find the chained assignment!!!\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# load the config\n",
    "# edit config directly in yaml file\n",
    "import yaml\n",
    "config_file = '../config/config_devel.yaml'\n",
    "def get_config(config_file, param):\n",
    "        with open(config_file) as file:\n",
    "        # The FullLoader parameter handles the conversion from YAML\n",
    "        # scalar values to Python the dictionary format\n",
    "            config = yaml.load(file, Loader=yaml.FullLoader)['CNV'][param]\n",
    "        return config\n",
    "config = get_config(config_file, 'combine')\n",
    "\n",
    "\n",
    "\n",
    "user = 'martinscience'\n",
    "# HOME\n",
    "home = '/Users/mahtin'\n",
    "home = '/Users/martinscience'\n",
    "\n",
    "# get the code\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from codeCNV.plot import plot_snp, plot_2d, plot_3d, plot_snp2\n",
    "from codeCNV.rollingSNP import apply_rolling_SNP\n",
    "from script_utils import show_output\n",
    "\n",
    "\n",
    "# user = 'mahtin'\n",
    "HOME = f\"/Users/{user}\"\n",
    "wes_path = f\"{HOME}/Dropbox/Icke/Work/somVar/AMLMono7/WESData\"\n",
    "path = f\"{wes_path}\"\n",
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T05:42:47.146048Z",
     "start_time": "2020-12-16T05:42:47.130857Z"
    }
   },
   "outputs": [],
   "source": [
    "from codeCNV.rollingCNV import interpolate, one_col_rolling, llh, get_blocks, rolling_data, get_CNV_blocks\n",
    "from script_utils import show_output\n",
    "\n",
    "def make_get_density(window_size=20):\n",
    "    '''\n",
    "    helper for returning a density computer for given window_size\n",
    "    '''\n",
    "    \n",
    "    def SNPdensity(data):\n",
    "        return (data.max() - data.min()) / window_size\n",
    "    return SNPdensity\n",
    "\n",
    "def remove_fallSNP(snp_df, mean=0.5, std=0.2, params={}):\n",
    "    '''\n",
    "    removes the falling SNP probably caused by mismapping\n",
    "    '''\n",
    "    \n",
    "    window = params['offVAFwindow']\n",
    "    cutoff = params['maxFallSNP']\n",
    "    \n",
    "    # get the density computer for rolling\n",
    "    get_SNPdensity = make_get_density(window)\n",
    "    # cycle through chroms\n",
    "    chrom_dfs = []\n",
    "    for chrom in snp_df['Chr'].unique():\n",
    "        df = snp_df.query('Chr == @chrom')\n",
    "  \n",
    "        # get the snp\n",
    "        df = one_col_rolling(df, df.query('VAF < 0.95'), 'ExonPos', get_SNPdensity, window_size=window, diff_exp=4)\n",
    "        df.loc[:, 'SNPdensity'] = df['SNPdensity'] / df['SNPdensity'].mean()\n",
    "    \n",
    "        # get the offVAFsum\n",
    "        df = one_col_rolling(df, df.query('VAF < 0.95'), 'offVAF', 'sum', window_size=window, normalize=True, diff_exp=4)\n",
    "    \n",
    "        # combine both metrices\n",
    "        df.loc[:, 'fallSNP'] = df['SNPdensity'] * df['offVAFsum']\n",
    "        # now remove the ones below average VAFstd\n",
    "        df = df.query('VAF > @mean - @std / 2 or fallSNP > @cutoff')\n",
    "        chrom_dfs.append(df)\n",
    "        \n",
    "    return pd.concat(chrom_dfs).sort_values('FullExonPos').reset_index(drop=True)\n",
    "\n",
    "\n",
    "def compute_snp_llh(df, mean=0.5, sigma=0.2):\n",
    "    '''\n",
    "    computes the local log-likelihood of belonging to the center gaussian\n",
    "    '''\n",
    "        \n",
    "    show_output(f\"Computing log-likelihood of VAF belonging to center gaussian [mean:{round(mean, 3)}, sigma:{round(sigma,3)}]\")\n",
    "    df.loc[:, 'snpLLH'] = llh(df['VAF'], mean, sigma)\n",
    "    \n",
    "    # for homoSNPs reduce the VAFs to the ones above mean\n",
    "    upper_vafs = df.query('@mean < VAF')['VAF']\n",
    "    # then compute the hsnpLLH\n",
    "\n",
    "    show_output(f\"Computing log-likelihood of VAF belonging to purity100  [mean:1, sigma:{round(sigma,3)}]\")\n",
    "    # these are called hsnp\n",
    "    # upper_vafs only contains half the snps, the remaining have to be interpolated\n",
    "    df.loc[:, 'hsnpLLH'] = llh(upper_vafs, 1, sigma)\n",
    "    df = interpolate(df, 'hsnpLLH', expand_limit=50)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def expand_SNPdata(snp_df, config):\n",
    "    '''\n",
    "    retrieve a few data columns locally to use rolling windows on\n",
    "    this needs to be done chromosome-wise in order to avoid gap effects\n",
    "    VAF limits are also applied here\n",
    "    '''\n",
    "        \n",
    "    # split the params dict for easier access\n",
    "    params = config['snp']\n",
    "    filter_params = params['filter']\n",
    "    # data_params = params['data']\n",
    "    \n",
    "    # reduce the snp_df using lower config limit\n",
    "    # upper limit has to be set later as we still need the homoSNP llh\n",
    "    VAFmin, VAFmax = filter_params['VAF']\n",
    "    snp_df = snp_df.query('@VAFmin < VAF')\n",
    "    \n",
    "    \n",
    "    # get std and mean of VAF\n",
    "    minVAF, maxVAF = params['LLH']['center_range']\n",
    "    # get the sigma and mean of the center band VAF (extracted as pd.Series center_vafs)\n",
    "    center_vafs = snp_df.query('@minVAF < VAF < @maxVAF')['VAF']\n",
    "    # get width of gaussian from std * sigma_factor\n",
    "    VAFstd = center_vafs.std()\n",
    "    VAFmean = center_vafs.mean()\n",
    "    \n",
    "    \n",
    "    # get additional features from VAFs\n",
    "    snp_df.loc[:, 'offVAF'] = (snp_df['VAF'] - VAFmean) * 2\n",
    "    # absolute values for cluster \n",
    "    snp_df.loc[:,'absVAF'] = np.abs(snp_df['offVAF'])    \n",
    "    \n",
    "    ########## remove fallSNP ########\n",
    "    fs_params = params['fallSNP']\n",
    "    if fs_params['run']:\n",
    "        show_output('Removing falling SNPs')\n",
    "        snp_df = remove_fallSNP(snp_df, mean=VAFmean, std=VAFstd, params=fs_params)\n",
    "     \n",
    "    ######## LLH  #####################\n",
    "    # get the snpLLH and hsnpLLH\n",
    "    # get config params\n",
    "    sigma = VAFstd * params['LLH']['sigma_factor']\n",
    "    # hsnpLLH is computed in order to rescue high absVAF that would have been filtered out\n",
    "    # lower VAF is already removed because density of VAF ~0 is highly irregular and would confound \n",
    "    snp_df = compute_snp_llh(snp_df, mean=VAFmean, sigma=sigma)\n",
    "\n",
    "    return snp_df.query('VAF < @VAFmax').reset_index(drop=True)\n",
    "\n",
    "def rolling_SNP(snp_df, config):\n",
    "    '''\n",
    "    cycle through the chroms and perform rolling window computations of snp data set in config\n",
    "    '''\n",
    "\n",
    "    # split the params dict for easier access\n",
    "    params = config['snp']\n",
    "    filter_params = params['filter']\n",
    "    data_params = params['rolling_data']\n",
    "    debug = config['debug']\n",
    "        \n",
    "    minDepth = filter_params['minDepth']\n",
    "    filter_df = snp_df.query('Depth >= @minDepth')\n",
    "    \n",
    "    rolling_df = rolling_data(snp_df, filter_df, expand=params['expand'], ddof=config['ddof'], debug=debug, data_params=data_params)\n",
    "    \n",
    "    return rolling_df\n",
    "\n",
    "def apply_rolling_SNP(snp_df, config):\n",
    "\n",
    "    # get extra data\n",
    "    snp_df = expand_SNPdata(snp_df, config)\n",
    "    # do the rolling\n",
    "    snp_df = rolling_SNP(snp_df, config)\n",
    "    # get the CNV and Center blocks\n",
    "    snp_df = get_CNV_blocks(snp_df, 'snpLLH', config)\n",
    "    \n",
    "    # select columns for output\n",
    "    base_cols = list(snp_df.columns[:4])\n",
    "\n",
    "    snp_cols = [col for col in snp_df.columns[4:] if not 'log2' in col and not 'cov' in col and not 'off' in col]\n",
    "    rolling_snp_df = snp_df[base_cols + snp_cols]\n",
    "    cluster_cols = ['log2ratio', 'log2ratiomean', 'VAF', 'absVAF', 'absVAFmean']\n",
    "    cluster_cols += [col for col in snp_df.columns if 'Center' in col or 'CNV' in col]\n",
    "    cluster_df = snp_df[base_cols + cluster_cols]\n",
    "    return rolling_snp_df, cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:31:05.648166Z",
     "start_time": "2020-12-14T13:31:05.386698Z"
    }
   },
   "outputs": [],
   "source": [
    "project = \"P483\"\n",
    "sample = \"01_A\"\n",
    "\n",
    "snp_df = pd.read_csv(os.path.join(wes_path, f\"CNVraw/{project}/{sample}.snp\"), sep='\\t')\n",
    "snp_df = expand_SNPdata(snp_df, config)\n",
    "snp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:40:48.268062Z",
     "start_time": "2020-12-14T13:40:47.744399Z"
    }
   },
   "outputs": [],
   "source": [
    "log2mean = dict(\n",
    "        title='rollinglog2ratio',\n",
    "        plot_type='line',   # ['line', 'scatter']\n",
    "        data='log2ratiomean',\n",
    "        plot_args=dict(\n",
    "            linewidth=1,\n",
    "            color='yellow',\n",
    "            alpha=.7\n",
    "        )\n",
    "    )\n",
    "\n",
    "log2 = dict(\n",
    "        title='log2ratio',\n",
    "        plot_type='scatter',   # ['line', 'scatter']\n",
    "        data='log2ratio',\n",
    "        plot_args=dict(\n",
    "            linewidth=0.3,\n",
    "            color='black',\n",
    "            cmap='binary',\n",
    "            # color='black',\n",
    "            s=2,\n",
    "            alpha=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "vaf = dict(\n",
    "        title='VAF',\n",
    "        plot_type='scatter',  # ['line', 'scatter']\n",
    "        data='VAF',\n",
    "        plot_args=dict(\n",
    "            s=1,\n",
    "            color='black',\n",
    "            cmap='viridis',\n",
    "            alpha=1\n",
    "        )\n",
    "    )\n",
    "chroms = ['chr3', 'chr4', 'chr5', 'chr6','chr7', 'chr9', 'chr12', 'chr17']\n",
    "r1 = 'chr17:3Mb-9Mb'\n",
    "\n",
    "######################################################\n",
    "\n",
    "fig_params = dict(\n",
    "    figsize=(24,5),\n",
    "    colormap='coolwarm_r',\n",
    "    color_chroms=True,\n",
    "    ylim=(-0,1),\n",
    "    cov_offset=.1,  # how much log2ratio=0 is shifted above SNP-data\n",
    "    cov_height=.5,\n",
    "    label_size=13\n",
    ")\n",
    "\n",
    "fig, _, _, _ = plot_snp2(snp_df.query('0.03 < VAF < 0.95'), snp_plots=[vaf], cov_plots=[log2,log2mean], chroms=\"all\", region='', **fig_params)\n",
    "#fig, ax, df, chrom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:39:36.473956Z",
     "start_time": "2020-12-08T11:39:35.361922Z"
    }
   },
   "outputs": [],
   "source": [
    "project = \"P483\"\n",
    "sample = \"01_A\"\n",
    "fig.savefig(os.path.join(wes_path, f\"CNVplots/svg/{project}/{sample}.svg\"))\n",
    "fig.savefig(os.path.join(wes_path, f\"CNVplots/jpg/{project}/{sample}.jpg\"), quality=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sample_df for all clustered samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T06:00:26.131138Z",
     "start_time": "2020-12-16T06:00:25.812425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P483/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P559/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P615/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P625/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P665/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P685/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/P778/CNV\n",
      "Looking for cluster files in /Users/martinscience/Dropbox/Icke/Work/somVar/AMLMono7/WESData/Pmerge/CNV\n"
     ]
    }
   ],
   "source": [
    "def get_sample_name(file):\n",
    "    sample = os.path.basename(file).split(\".\")[0]\n",
    "    name = sample.split('_')[0] + sample.split('_')[1].replace(\"-B\", \"\")\n",
    "    return name\n",
    "\n",
    "\n",
    "projects = [f\"P{i}\" for i in [483,559,615,625,665,685,778,\"merge\"]]\n",
    "\n",
    "sample_df = pd.DataFrame(columns=['sample', 'file'])\n",
    "for project in projects:\n",
    "    folder = os.path.join(wes_path, f\"{project}/CNV\")\n",
    "    print(f\"Looking for cluster files in {folder}\")\n",
    "    for folder, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".cluster\"):\n",
    "                sample = file.replace(\".cluster\", \"\").replace(\"_\", \"\")\n",
    "                file = os.path.join(folder, file)\n",
    "                # print(f\"Found sample {sample} as file {file}\")\n",
    "                sample_df = sample_df.append(pd.Series(dict(sample=sample, project=project, file=file)), ignore_index=True)\n",
    "sample_df.to_csv(os.path.join(wes_path, \"cluster_samples.csv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making plots for the merge check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:44:05.298493Z",
     "start_time": "2020-12-14T13:43:03.057861Z"
    }
   },
   "outputs": [],
   "source": [
    "projects = [f\"P{i}\" for i in [\"12\"]]\n",
    "\n",
    "def get_sample_name(file):\n",
    "    sample = os.path.basename(file).split(\".\")[0]\n",
    "    name = sample.split('_')[0] + sample.split('_')[1].replace(\"-B\", \"\")\n",
    "    return name\n",
    "\n",
    "\n",
    "sample_list = []\n",
    "filter_lists = []\n",
    "for project in projects:\n",
    "    folder = os.path.join(wes_path, f\"CNVraw/{project}\")\n",
    "    print(f\"Looking for filter files in {folder}\")\n",
    "    for folder, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".snp\"):\n",
    "                sample = file.replace(\".snp\", \"\").replace(\"_\", \"\")\n",
    "                print(sample)\n",
    "                # if sample != \"27B\" and sample !=\"30R\":\n",
    "                    # continue\n",
    "                print(f\"Found sample {sample} as file {file}\")\n",
    "                # load sample\n",
    "                snp_df = pd.read_csv(os.path.join(folder, file), sep='\\t')\n",
    "                snp_df = expand_SNPdata(snp_df, config)\n",
    "                fig, _, _, _ = plot_snp2(snp_df.query('0.03 < VAF < 0.95'), snp_plots=[vaf], cov_plots=[log2,log2mean], chroms=\"all\", region='', **fig_params)\n",
    "                fig.savefig(os.path.join(wes_path, f\"CNVplots/svg/{project}/{sample}.svg\"))\n",
    "                fig.savefig(os.path.join(wes_path, f\"CNVplots/jpg/{project}/{sample}.jpg\"), quality=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making plots for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T11:47:46.643Z"
    }
   },
   "outputs": [],
   "source": [
    "projects = [f\"P{i}\" for i in [483,559,615,625,665,685,778,\"merge\"]]\n",
    "\n",
    "def get_sample_name(file):\n",
    "    sample = os.path.basename(file).split(\".\")[0]\n",
    "    name = sample.split('_')[0] + sample.split('_')[1].replace(\"-B\", \"\")\n",
    "    return name\n",
    "\n",
    "\n",
    "sample_list = []\n",
    "filter_lists = []\n",
    "for project in projects:\n",
    "    folder = os.path.join(wes_path, f\"CNVraw/{project}\")\n",
    "    print(f\"Looking for filter files in {folder}\")\n",
    "    for folder, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".snp\"):\n",
    "                sample = file.replace(\".snp\", \"\").replace(\"_\", \"\")\n",
    "                print(f\"Found sample {sample} as file {file}\")\n",
    "                # load sample\n",
    "                snp_df = pd.read_csv(os.path.join(folder, file), sep='\\t')\n",
    "                snp_df = expand_SNPdata(snp_df, config)\n",
    "                fig, _, _, _ = plot_snp2(snp_df.query('0.03 < VAF < 0.95'), snp_plots=[vaf], cov_plots=[log2,log2mean], chroms=\"all\", region='', **fig_params)\n",
    "                fig.savefig(os.path.join(wes_path, f\"CNVplots/svg/{project}/{sample}.svg\"))\n",
    "                fig.savefig(os.path.join(wes_path, f\"CNVplots/jpg/{project}/{sample}.jpg\"), quality=90)\n",
    "[get_sample_name(s) for s in sample_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running rollingSNP for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T16:26:02.789723Z",
     "start_time": "2020-12-09T16:25:47.649042Z"
    }
   },
   "outputs": [],
   "source": [
    "projects = [f\"P{i}\" for i in [483,559,615,625,665,685,778,\"merge\"]]\n",
    "\n",
    "def get_sample_name(file):\n",
    "    sample = os.path.basename(file).split(\".\")[0]\n",
    "    name = sample.split('_')[0] + sample.split('_')[1].replace(\"-B\", \"\")\n",
    "    return name\n",
    "\n",
    "\n",
    "sample_list = []\n",
    "filter_lists = []\n",
    "for project in projects:\n",
    "    folder = os.path.join(wes_path, f\"CNVraw/{project}\")\n",
    "    print(f\"Looking for filter files in {folder}\")\n",
    "    for folder, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".snp\") and not \"roll\" in file:\n",
    "                sample = file.replace(\".snp\", \"\").replace(\"_\", \"\")\n",
    "                if sample != \"27B\" and sample !=\"30R\":\n",
    "                    continue\n",
    "                print(f\"Found sample {sample} as file {file}\")\n",
    "                # load sample\n",
    "                in_file = os.path.join(folder, file)\n",
    "                snp_df = pd.read_csv(in_file, sep='\\t')\n",
    "                snp_df, cluster_df = apply_rolling_SNP(snp_df, config)\n",
    "                out_file = os.path.join(wes_path, f\"{project}/CNV/{file}\")\n",
    "                roll_file = out_file.replace(\".snp\", \".roll.snp\")\n",
    "                snp_df.to_csv(roll_file, sep='\\t', index=False)\n",
    "                cluster_file = out_file.replace(\".snp\", \".cluster\")\n",
    "                cluster_df.to_csv(cluster_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making sample_df for all clustered samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
